global:
  kubeconfig: ""
  create_index:
    enabled: true
    image: "alpine:3.22"
  persistence:
    enabled: false # if true = Enable the creation of PVC when duckdb is used - Recommended TRUE
    dynamic: false # if true = let the PVCs create PVs if possible and do not create them manually
    accessModes: "ReadWriteOnce"
    reclaimPolicy: "Retain"
    storageClass: "local-path" # if empty = default storage class
    hostPath: "/mnt/fred-data" # only used if dynamic=false & storageClass=local-path, remind to create this directory on your host
  whitelist:
    enabled: false
    configMapName: ""
    filePath: "files/whitelist/users.txt"
    user_list_inline: ""
    mountPath: /fred-core/fred_core/security/whitelist_access_control/users.txt
    subPath: users.txt

applications:
  agentic-backend:
    enabled: true
    applicationName: agentic-backend
    deployment:
      enabled: true
    replicaCount: 1
    statefulset:
      enabled: false
    job:
      enabled: false
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    image:
      repository: ghcr.io/thalesgroup/fred-agent/agentic-backend
      tag: "v1.0.10"
      pullPolicy: IfNotPresent
    command:
      enabled: true
      data:
        - "uvicorn"
        - "agentic_backend.main:create_app"
        - "--factory"
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "8000"
        - "--log-level"
        - "info"
        - "--loop"
        - "asyncio"
    env:
      - name: CONFIG_FILE
        value: "/app/config/configuration.yaml"
    ports:
      - name: http
        containerPort: 8000
      - name: metrics
        containerPort: 9000
    service:
      annotations: {}
      extraLabels: {}
      type: ClusterIP
      ports:
        - name: http
          port: 80
          targetPort: 8000
    metricsService:
      enabled: true
      annotations: {}
      extraLabels: {}
      type: ClusterIP
      port: 9000
      targetPort: 9000
      serviceMonitor:
        enabled: false
        annotations: {}
        labels: {}
        interval: 30s
        scrapeTimeout: 10s
        path: /metrics
        scheme: http
        honorLabels: false
        relabelings: []
        metricRelabelings: []
        namespaceSelector: {}
    ingress:
      enabled: false
      className: ""
      hosts: []
    volumeMounts:
      - name: agentic-backend-vol
        mountPath: /app/config/configuration.yaml
        subPath: configuration.yaml
      - name: agentic-backend-env-vol
        mountPath: /app/config/.env
        subPath: .env
      - name: agentic-backend-kube-vol
        mountPath: /home/fred-user/.kube/config
        subPath: kubeconfig
    volumes:
      - name: agentic-backend-vol
        configMap:
          name: agentic-backend-back
      - name: agentic-backend-env-vol
        secret:
          secretName: agentic-backend-env
      - name: agentic-backend-kube-vol
        configMap:
          name: agentic-backend-kube
          items:
            - key: kubeconfig
              path: kubeconfig
    probes:
      lifecycle:
        enabled: false
      livenessProbe:
        enabled: true
        data:
          httpGet:
            path: /agentic/v1/healthz
            port: 8000
      readinessProbe:
        enabled: true
        data:
          httpGet:
            path: /agentic/v1/ready
            port: 8000
      startupProbe:
        enabled: false
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - "ALL"
      runAsUser: 1000
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount:
      annotations: {}
      labels: {}
      automountServiceAccountToken: true
      rbac:
        enabled: true
        permissions:
          namespaced:
            - apiGroups: [""]
              resources: ["pods", "configmaps", "secrets"]
              verbs: ["get", "list", "watch"]
            - apiGroups: ["apps"]
              resources: ["deployments", "replicasets"]
              verbs: ["get", "list", "watch", "create", "update", "patch"]
            - apiGroups: [""]
              resources: ["events"]
              verbs: ["create"]
          cluster:
            - apiGroups: [""]
              resources: ["nodes"]
              verbs: ["get", "list", "watch"]
    kubeconfig:
      enabled: true
    configuration_type:
      backend: true
      frontend: false
    configuration:
      app:
        name: "Agentic Backend"
        base_url: "/agentic/v1"
        address: "127.0.0.1"
        port: 8000
        log_level: "info"
        reload: false
        reload_dir: "."
        metrics_enabled: true # Prometheus exporter
        metrics_address: "0.0.0.0"
        metrics_port: 9000
        kpi_process_metrics_interval_sec: 0
        kpi_log_summary_interval_sec: 0.0
        kpi_log_summary_top_n: 0

      security:
        m2m:
          enabled: true
          client_id: "agentic"
          realm_url: "http://idp.dev.fred.thalesgroup.com/realms/app"
        user:
          enabled: true
          client_id: "app"
          realm_url: "http://idp.dev.fred.thalesgroup.com/realms/app"
        authorized_origins:
          - "http://fred.dev.fred.thalesgroup.com:80"
        rebac:
          enabled: false                    # Set false to bypass ReBAC (warning: all private resources will become public)
          type: openfga
          api_url: "http://openfga:8080"    # OpenFGA HTTP endpoint
          # store_name: "fred"                # OpenFGA store name. Reuse the same store across services
          # create_store_if_needed: true      # Automates store bootstrap (disable if pre-provisioned)
          # sync_schema_on_init: true         # Pushes the default Fred authorization model
          # authorization_model_id: null      # Authorization model id to use in case `sync_schema_on_init: false`
          # token_env_var: "OPENFGA_API_TOKEN" # Env var holding the bearer token
          # timeout_millisec: 2000            # Optional request timeout
          # headers:
          #   # Optional static headers sent to OpenFGA
          #   X-Custom-Header: "value"
      frontend_settings:
        feature_flags:
          # If true activate the backend and frontend modules in charge of K8
          # and frugality monitoring
          enableK8Features: false
      scheduler:
        # Scheduler backend driving agent workflows.
        # - "memory": in-process queue for local/dev.
        # - "temporal": use Temporal Cloud/self-hosted to run agent tasks asynchronously.
        backend: "temporal"
        temporal:
          # Temporal frontend endpoint and namespace used by the agentic worker.
          host: "temporal.default.svc.cluster.local:7233"
          namespace: "default"
          task_queue: "agents"
          workflow_id_prefix: "task"
          connect_timeout_seconds: 5
        # Optional progress query name and memo/search attributes can be added here if your
        # cluster uses custom Temporal conventions.
          # If true activate support for an electronic warfare demonstration
          enableElecWarfare: false
        properties:
          logoName: "fred"
      ai:
        # In production  activate the configuration persistence mechanism
        # that allow users to create/update agents at runtime and change
        # their configuration without redeploying the application.
        use_static_config_only: false
        # Number of past exchanges to restore when initializing an agent session
        # This setting is used to limit the amount of historical context
        # that is loaded into the agent at the beginning of a session. It typically occur after a restart of the application
        # or when an agent session is re-initialized for any reason.
        agentic_restore_max_exchanges: 20

        # Maximum number of agents that cached in memory for faster access (uses LRU eviction policy)
        max_concurrent_agents: 1024

        # Maximum number of concurrent sessions per user. This is used to prevent abuse and manage resource usage.
        max_concurrent_sessions_per_user: 10

        # Maximum number of files a user can attach across all sessions
        max_attached_files_per_user: 20

        # Maximum size (in MB) for each attached file
        max_attached_file_size_mb: 50

        # Base URL for the Knowledge Flow service
        knowledge_flow_url: "http://knowledge-flow-backend:8111/knowledge-flow/v1"
        # Timeout settings for the client
        timeout:
          connect: 5 # Time to wait for a connection in seconds
          read: 15 # Time to wait for a response in seconds
        default_chat_model:
          provider: "openai"
          name: "gpt-4o-mini"
          settings:
            temperature: 0.0
            max_retries: 1
            timeout:
              connect: 10.0
              read: 120.0
              write: 30.0
              pool: 5.0
            http_client_limits:
              max_connections: 500
              max_keepalive_connections: 200
              keepalive_expiry_seconds: 10
        default_language_model:
          provider: "openai"
          name: "gpt-4o"
          settings:
            temperature: 0.0
            max_retries: 1
            timeout:
              connect: 10.0
              read: 180.0
              write: 30.0
              pool: 5.0
            http_client_limits:
              max_connections: 500
              max_keepalive_connections: 200
              keepalive_expiry_seconds: 10
          crossencoder_model:
            name: cross-encoder/ms-marco-MiniLM-L-12-v2
            settings:
              online: false
              local_path: /app/models

        recursion:
          recursion_limit: 40 # Number or max recursion use by the agents while using the chat model
        agents:
          - id: "Samy"
            name: "Samy"
            type: "agent"
            class_path: "agentic_backend.agents.sentinel.sentinel_expert.SentinelExpert"
            enabled: false
          - id: "Georges"
            name: "Georges"
            type: "agent"
            class_path: "agentic_backend.agents.generalist.generalist_expert.Georges"
            enabled: true
            chat_options:
              search_policy_selection: false
              libraries_selection: false
              attach_files: true
          - id: "Tessa"
            name: "Tessa"
            type: "agent"
            class_path: "agentic_backend.agents.tabular.tabular_expert.Tessa"
            enabled: true
          - id: "Rico"
            name: "Rico"
            type: "agent"
            class_path: "agentic_backend.agents.rags.rag_expert.Rico"
            enabled: true
            chat_options:
              search_policy_selection: true
              libraries_selection: true
              search_rag_scoping: true
              attach_files: true
          - id: "Rico Senior"
            name: "Rico Senior"
            type: "agent"
            role: "Advanced Document Retrieval Expert"
            class_path: "agentic_backend.agents.rags.advanced_rag_expert.AdvancedRico"
            enabled: false
      mcp:
        servers:
          - id: "mcp-knowledge-flow-mcp-tabular"
            name: "mcp.servers.tabular.name"
            description: "mcp.servers.tabular.description"
            transport: "streamable_http"
            url: "http://knowledge-flow-backend:8000/knowledge-flow/v1/mcp-tabular"
            sse_read_timeout: 2000
            auth_mode: "user_token"
          - id: "mcp-knowledge-flow-opensearch-ops"
            name: "mcp.servers.search_opensearch.name"
            description: "mcp.servers.search_opensearch.description"
            transport: "streamable_http"
            url: "http://knowledge-flow-backend:8000/knowledge-flow/v1/mcp-opensearch-ops"
            sse_read_timeout: 2000
            auth_mode: "user_token"
          - id: "mcp-kubernetes-server"
            name: "mcp.servers.kubernetes.name"
            description: "mcp.servers.kubernetes.description"
            enabled: false
            transport: "streamable_http"
            url: "http://mcp-kubernetes-server:8081/mcp"
            sse_read_timeout: 2000
            auth_mode: "user_token"
      storage:
        postgres:
          host: localhost
          port: 5432
          database: fred
          username: admin
        opensearch:
          host: https://localhost:9200
          secure: true
          verify_certs: false
          username: admin
        feedback_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/agentic/feedback.duckdb"

        agent_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/agentic/agent.duckdb"

        mcp_servers_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/agentic/mcp_servers.duckdb"

        session_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/agentic/session.duckdb"

        history_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/agentic/history.duckdb"

        attachments_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/agentic/session_attachments.duckdb"

        kpi_store:
          type: "log"
          level: "INFO"

    dotenv:
      AZURE_OPENAI_API_KEY: ""
      KEYCLOAK_AGENTIC_CLIENT_SECRET: ""
      OPENAI_API_KEY: ""
      OPENSEARCH_PASSWORD: ""
      M2M_CLIENT_SECRET: ""
      OPENFGA_API_TOKEN: ""
      FRED_POSTGRES_PASSWORD: ""

  frontend:
    enabled: true
    applicationName: frontend
    deployment:
      enabled: true
    replicaCount: 1
    statefulset:
      enabled: false
    job:
      enabled: false
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    image:
      repository: ghcr.io/thalesgroup/fred-agent/frontend
      tag: "v1.0.10"
      pullPolicy: IfNotPresent
    command:
      enabled: false
    env:
      - name: VITE_ALLOWED_HOSTS
        value: "fred.dev"
      - name: VITE_BACKEND_URL_KNOWLEDGE
        value: "http://knowledge-flow-backend.dev.fred.thalesgroup.com"
      - name: VITE_USE_AUTH
        value: "true"
    ports:
      - name: http
        containerPort: 8080
    service:
      annotations: {}
      extraLabels: {}
      type: ClusterIP
      ports:
        - name: http
          port: 80
          targetPort: 8080
    ingress:
      enabled: true
      className: "nginx"
      hosts:
        - host: fred.dev.fred.thalesgroup.com
          paths:
            - path: /
              pathType: Prefix
              service:
                name: frontend
                port: 80
            - path: /agentic
              pathType: Prefix
              service:
                name: agentic-backend
                port: 80
            - path: /knowledge-flow
              pathType: Prefix
              service:
                name: knowledge-flow-backend
                port: 8000
      tls:
        - secretName: frontend-crt
          hosts:
            - fred.dev.fred.thalesgroup.com
    volumeMounts:
      - name: frontend-config-vol
        mountPath: /usr/share/nginx/html/config.json
        subPath: config.json
      - name: frontend-config-vol
        mountPath: /usr/share/nginx/html/keycloak.json
        subPath: keycloak.json
    volumes:
      - name: frontend-config-vol
        configMap:
          name: frontend-front
    probes:
      lifecycle:
        enabled: true
      livenessProbe:
        enabled: true
        data:
          httpGet:
            path: /healthz.html
            port: 8080
      readinessProbe:
        enabled: true
        data:
          httpGet:
            path: /ready.html
            port: 8080
      startupProbe:
        enabled: false
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - "ALL"
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount:
      annotations: {}
      labels: {}
      automountServiceAccountToken: true
      rbac:
        enabled: true
        permissions: {}
    kubeconfig:
      enabled: false
    configuration_type:
      frontend: true
      backend: false
    configuration:
      config_json:
        backend_url_api: "http://fred.dev.fred.thalesgroup.com"
        backend_url_knowledge: "https://fred.dev.fred.thalesgroup.com"
        websocket_url: "ws://fred.dev.fred.thalesgroup.com/fred/chatbot/query"
        frontend_basename : "/"

  knowledge-flow-backend:
    enabled: true
    applicationName: knowledge-flow-backend
    configurationFileName: configuration.yaml
    deployment:
      enabled: true
    replicaCount: 1
    statefulset:
      enabled: false
    job:
      enabled: false
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    image:
      repository: ghcr.io/thalesgroup/fred-agent/knowledge-flow-backend
      tag: "v1.0.10"
    command:
      enabled: true
      data:
        - "uvicorn"
        - "knowledge_flow_backend.main:create_app"
        - "--factory"
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "8111"
        - "--log-level"
        - "info"
        - "--loop"
        - "asyncio"
    env:
      - name: LOG_LEVEL
        value: "INFO"
      - name: CONFIG_FILE
        value: "/app/config/configuration.yaml"
    ports:
      - name: http
        containerPort: 8111
      - name: metrics
        containerPort: 9111
      - name: https
        containerPort: 8443
    service:
      annotations: {}
      extraLabels: {}
      type: ClusterIP
      ports:
        - name: "http"
          port: 8000
          targetPort: 8111
          protocol: TCP
        - name: "https"
          port: 8443
          targetPort: 8443
          protocol: TCP
    metricsService:
      enabled: true
      annotations: {}
      extraLabels: {}
      type: ClusterIP
      port: 9111
      targetPort: 9111
      serviceMonitor:
        enabled: false
        annotations: {}
        labels: {}
        interval: 30s
        scrapeTimeout: 10s
        path: /metrics
        scheme: http
        honorLabels: false
        relabelings: []
        metricRelabelings: []
        namespaceSelector: {}
    ingress:
      enabled: false
      className: ""
      hosts: []
    volumeMounts:
      - name: knowledge-flow-backend-vol
        mountPath: /app/config/configuration.yaml
        subPath: configuration.yaml
      - name: knowledge-flow-backend-env-vol
        mountPath: /app/config/.env
        subPath: .env
    volumes:
      - name: knowledge-flow-backend-vol
        configMap:
          name: knowledge-flow-backend-back
      - name: knowledge-flow-backend-env-vol
        secret:
          secretName: knowledge-flow-backend-env
    probes:
      lifecycle:
        enabled: false
      livenessProbe:
        enabled: true
        data:
          failureThreshold: 10
          httpGet:
            path: /knowledge-flow/v1/healthz
            port: 8111
          periodSeconds: 10
      readinessProbe:
        enabled: true
        data:
          failureThreshold: 10
          httpGet:
            path: /knowledge-flow/v1/ready
            port: 8111
          periodSeconds: 10
      startupProbe:
        enabled: false
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - "ALL"
      runAsUser: 1000
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount:
      annotations: {}
      labels: {}
      automountServiceAccountToken: true
      rbac:
        enabled: true
        permissions: {}
    kubeconfig:
      enabled: false
    configuration_type:
      backend: true
      frontend: false
    configuration:
      app:
        name: "Knowledge Flow Backend"
        base_url: "/knowledge-flow/v1"
        address: "127.0.0.1"
        port: 8111
        log_level: "info"
        reload: false
        reload_dir: "."
        metrics_enabled: true # Prometheus exporter
        metrics_address: "0.0.0.0"
        metrics_port: 9111
        kpi_process_metrics_interval_sec: 0
        kpi_log_summary_interval_sec: 0.0
        kpi_log_summary_top_n: 0
        max_ingestion_workers: 3

      processing:
        generate_summary: true
        use_gpu: true
        process_images: true
      security:
        m2m:
          enabled: true
          client_id: "knowledge-flow"
          realm_url: "http://idp.dev.fred.thalesgroup.com/realms/app"
          secret_env_var: KEYCLOAK_KNOWLEDGE_FLOW_CLIENT_SECRET
        user:
          enabled: true
          client_id: "app"
          realm_url: "http://idp.dev.fred.thalesgroup.com/realms/app"
        authorized_origins:
          - "http://fred.dev.fred.thalesgroup.com:80"
        rebac:
          enabled: false                    # Set false to bypass ReBAC (warning: all private resources will become public)
          type: openfga
          api_url: "http://openfga:8080"    # OpenFGA HTTP endpoint
          # store_name: "fred"                # OpenFGA store name. Reuse the same store across services
          # create_store_if_needed: true      # Automates store bootstrap (disable if pre-provisioned)
          # sync_schema_on_init: true         # Pushes the default Fred authorization model
          # authorization_model_id: null      # Authorization model id to use in case `sync_schema_on_init: false`
          # token_env_var: "OPENFGA_API_TOKEN" # Env var holding the bearer token
          # timeout_millisec: 2000            # Optional request timeout
          # headers:
          #   # Optional static headers sent to OpenFGA
          #   X-Custom-Header: "value"
      scheduler:
        enabled: true
        backend: "temporal"
        temporal:
          host: "localhost:7233"
          namespace: "default"
          task_queue: "ingestion"
          workflow_id_prefix: "pipeline"
          connect_timeout_seconds: 5
      input_processors:
        - prefix: ".pdf"
          class_path: knowledge_flow_backend.core.processors.input.pdf_markdown_processor.pdf_markdown_processor.PdfMarkdownProcessor
        - prefix: ".docx"
          class_path: knowledge_flow_backend.core.processors.input.docx_markdown_processor.docx_markdown_processor.DocxMarkdownProcessor
        - prefix: ".pptx"
          class_path: knowledge_flow_backend.core.processors.input.pptx_markdown_processor.pptx_markdown_processor.PptxMarkdownProcessor
        - prefix: ".csv"
          class_path: knowledge_flow_backend.core.processors.input.csv_tabular_processor.csv_tabular_processor.CsvTabularProcessor
        - prefix: ".txt"
          class_path: knowledge_flow_backend.core.processors.input.text_markdown_processor.text_markdown_processor.TextMarkdownProcessor
        - prefix: ".md"
          class_path: knowledge_flow_backend.core.processors.input.markdown_markdown_processor.markdown_markdown_processor.MarkdownMarkdownProcessor
        - prefix: ".xlsm"
          class_path: knowledge_flow_backend.core.processors.input.pps_tabular_processor.pps_tabular_processor.PpsTabularProcessor
        - prefix: ".jsonl"
          class_path: knowledge_flow_backend.core.processors.input.jsonl.jsonl_markdown_processor.JsonlMarkdownProcessor
      content_storage:
        type: local
        root_path: /home/fred-user/.fred/knowledge-flow/content-storage
        # type: minio
        # endpoint: http://minio:9000
        # access_key: admin
        # bucket_name: app-bucket
        # secure: false
        # public_endpoint: https://minio.example.com  # Public URL for browser-facing presigned URLs
        # public_secure: true  # Optional, inferred from public_endpoint scheme if omitted
      document_sources:
        fred:
          type: push
          description: "Documents manually uploaded by users"
        local:
          type: pull
          provider: local_path
          base_path: ~/Documents/Fred
          description: "Personal local documents available for pull-mode ingestion"
      embedding_model:
        provider: openai
        name: text-embedding-3-large
        settings: {}
        type: "openai"
      chat_model:
        provider: openai # openai | azure | ollama
        name: gpt-4o-mini # azure: deployment name, ollama: 'qwen2.5:3b-instruct' etc.
        settings: {}
          # openai: nothing else required (keys in .env)
          # azure: { endpoint: "...", api_version: "2024-08-01-preview" }
          # ollama: { base_url: "http://localhost:11434"
      vision_model:
        provider: openai
        name: gpt-4o-mini # or gpt-4o if you want higher fidelity
        settings:
          temperature: 0 # optional
          max_retries: 2 # optional
      storage:
        postgres:
          host: localhost
          port: 5432
          database: fred
          username: admin
        opensearch:
          host: https://localhost:9200
          secure: true
          verify_certs: false
          username: admin
        catalog_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/knowledge-flow/catalog.duckdb"
        prompt_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/knowledge-flow/prompt.duckdb"
        resource_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/knowledge-flow/resource.duckdb"
        kpi_store:
          type: "log"
          level: "INFO"
        tag_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/knowledge-flow/tag.duckdb"
        metadata_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/knowledge-flow/metadata.duckdb"
        task_store:
          type: "postgres"
          table: workflow_tasks
          prefix: "sched_"

        vector_store:
          type: in_memory
        tabular_stores:
          base_database: # Minimal configuration to enable CSV document ingestion
            type: "sql"
            driver: "duckdb"
            database: "base_database"
            path: "/home/fred-user/.fred/knowledge-flow/db.duckdb"
            mode: "read_and_write"
      mcp:
        reports_enabled: true
        kpi_enabled: true
        tabular_enabled: true
        statistic_enabled: true
        text_enabled: true
        templates_enabled: true
        resources_enabled: true
        filesystem_enabled: false
        opensearch_ops_enabled: false
        neo4j_enabled: false

      filesystem:
        type: local
        # type: minio
        # endpoint: http://localhost:9000
        # access_key: admin
        # bucket_name: filesystem
        # secure: false

    dotenv:
      KEYCLOAK_KNOWLEDGE_FLOW_CLIENT_SECRET: ""
      MINIO_SECRET_KEY: ""
      OPENAI_API_KEY: ""
      OPENSEARCH_PASSWORD: ""
      OPENFGA_API_TOKEN: ""
      FRED_POSTGRES_PASSWORD: ""
      TABULAR_POSTGRES_PASSWORD: ""

  knowledge-flow-worker:
    # Toggle Temporal worker pod deployment.
    # Note: You need to have set knowledge-flow-backend.configuration.scheduler.enabled = true
    # otherwise this deployment will not be created.
    enabled: false
    applicationName: knowledge-flow-worker
    configurationFileName: configuration_worker.yaml
    deployment:
      enabled: false
    replicaCount: 3 # 3 temporal workers
    statefulset:
      enabled: false
    job:
      enabled: false
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    image:
      repository: ghcr.io/thalesgroup/fred-agent/knowledge-flow-backend
      tag: "v1.0.10"
    command:
      enabled: true
      data:
        - "python"
        - "-m"
        - "knowledge_flow_backend.main_worker"
    env:
      - name: LOG_LEVEL
        value: "INFO"
      - name: ENV_FILE
        value: "/app/config/.env"
      - name: CONFIG_FILE
        value: "/app/config/configuration_worker.yaml"
    ports:
      - name: worker
        containerPort: 8112
    service:
      enabled: false
      annotations: {}
      extraLabels: {}
      type: ClusterIP
      ports:
        - name: worker
          port: 8112
          targetPort: 8112
          protocol: TCP
    metricsService:
      enabled: false
      annotations: {}
      extraLabels: {}
      type: ClusterIP
      port: 9111
      targetPort: 9111
      serviceMonitor:
        enabled: false
        annotations: {}
        labels: {}
        interval: 30s
        scrapeTimeout: 10s
        path: /metrics
        scheme: http
        honorLabels: false
        relabelings: []
        metricRelabelings: []
        namespaceSelector: {}
    ingress:
      enabled: false
      className: ""
      hosts: []
    volumeMounts:
      - name: knowledge-flow-worker-vol
        mountPath: /app/config/configuration_worker.yaml
        subPath: configuration_worker.yaml
      - name: knowledge-flow-worker-env-vol
        mountPath: /app/config/.env
        subPath: .env
    volumes:
      - name: knowledge-flow-worker-vol
        configMap:
          name: knowledge-flow-worker-back
      - name: knowledge-flow-worker-env-vol
        secret:
          secretName: knowledge-flow-backend-env
    probes:
      lifecycle:
        enabled: false
      livenessProbe:
        enabled: false
        data: {}
      readinessProbe:
        enabled: false
        data: {}
      startupProbe:
        enabled: false
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - "ALL"
      runAsUser: 1000
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount:
      annotations: {}
      labels: {}
      automountServiceAccountToken: true
      rbac:
        enabled: true
        permissions: {}
    kubeconfig:
      enabled: false
    configuration_type:
      backend: true
      frontend: false
    configuration:
      app:
        name: "Knowledge Flow Backend"
        base_url: "/knowledge-flow/v1"
        address: "127.0.0.1"
        port: 8112
        log_level: "info"
        reload: false
        reload_dir: "."
        metrics_enabled: false # Do not export metrics since its a worker pod
        metrics_address: "0.0.0.0"
        metrics_port: 9111
        kpi_process_metrics_interval_sec: 0
        kpi_log_summary_interval_sec: 0.0
        kpi_log_summary_top_n: 0

      processing:
        generate_summary: true
        use_gpu: true
        process_images: true
      security:
        m2m:
          enabled: true
          client_id: "knowledge-flow"
          realm_url: "http://idp.dev.fred.thalesgroup.com/realms/app"
          secret_env_var: KEYCLOAK_KNOWLEDGE_FLOW_CLIENT_SECRET
        user:
          enabled: true
          client_id: "app"
          realm_url: "http://idp.dev.fred.thalesgroup.com/realms/app"
        authorized_origins:
          - "http://fred.dev.fred.thalesgroup.com:80"
        rebac:
          enabled: false                    # Set false to bypass ReBAC (warning: all private resources will become public)
          type: openfga
          api_url: "http://openfga:8080"    # OpenFGA HTTP endpoint
          # store_name: "fred"                # OpenFGA store name. Reuse the same store across services
          # create_store_if_needed: true      # Automates store bootstrap (disable if pre-provisioned)
          # sync_schema_on_init: true         # Pushes the default Fred authorization model
          # authorization_model_id: null      # Authorization model id to use in case `sync_schema_on_init: false`
          # token_env_var: "OPENFGA_API_TOKEN" # Env var holding the bearer token
          # timeout_millisec: 2000            # Optional request timeout
          # headers:
          #   # Optional static headers sent to OpenFGA
          #   X-Custom-Header: "value"
      scheduler:
        enabled: true
        backend: "temporal"
        temporal:
          host: "localhost:7233"
          namespace: "default"
          task_queue: "ingestion"
          workflow_id_prefix: "pipeline"
          connect_timeout_seconds: 5
      input_processors:
        - prefix: ".pdf"
          class_path: knowledge_flow_backend.core.processors.input.pdf_markdown_processor.pdf_markdown_processor.PdfMarkdownProcessor
        - prefix: ".docx"
          class_path: knowledge_flow_backend.core.processors.input.docx_markdown_processor.docx_markdown_processor.DocxMarkdownProcessor
        - prefix: ".pptx"
          class_path: knowledge_flow_backend.core.processors.input.pptx_markdown_processor.pptx_markdown_processor.PptxMarkdownProcessor
        - prefix: ".csv"
          class_path: knowledge_flow_backend.core.processors.input.csv_tabular_processor.csv_tabular_processor.CsvTabularProcessor
        - prefix: ".txt"
          class_path: knowledge_flow_backend.core.processors.input.text_markdown_processor.text_markdown_processor.TextMarkdownProcessor
        - prefix: ".md"
          class_path: knowledge_flow_backend.core.processors.input.markdown_markdown_processor.markdown_markdown_processor.MarkdownMarkdownProcessor
        - prefix: ".xlsm"
          class_path: knowledge_flow_backend.core.processors.input.pps_tabular_processor.pps_tabular_processor.PpsTabularProcessor
        - prefix: ".jsonl"
          class_path: knowledge_flow_backend.core.processors.input.jsonl.jsonl_markdown_processor.JsonlMarkdownProcessor
      content_storage:
        type: local
        root_path: /home/fred-user/.fred/knowledge-flow/content-storage
        # type: minio
        # endpoint: http://minio:9000
        # access_key: admin
        # bucket_name: app-bucket
        # secure: false
        # public_endpoint: https://minio.example.com  # Public URL for browser-facing presigned URLs
        # public_secure: true  # Optional, inferred from public_endpoint scheme if omitted
      document_sources:
        fred:
          type: push
          description: "Documents manually uploaded by users"
        local:
          type: pull
          provider: local_path
          base_path: ~/Documents/Fred
          description: "Personal local documents available for pull-mode ingestion"
      embedding_model:
        provider: openai
        name: text-embedding-3-large
        settings: {}
        type: "openai"
      chat_model:
        provider: openai # openai | azure | ollama
        name: gpt-4o-mini # azure: deployment name, ollama: 'qwen2.5:3b-instruct' etc.
        settings: {}
          # openai: nothing else required (keys in .env)
          # azure: { endpoint: "...", api_version: "2024-08-01-preview" }
          # ollama: { base_url: "http://localhost:11434"
      vision_model:
        provider: openai
        name: gpt-4o-mini # or gpt-4o if you want higher fidelity
        settings:
          temperature: 0 # optional
          max_retries: 2 # optional
      storage:
        postgres:
          host: localhost
          port: 5432
          database: fred
          username: admin
        opensearch:
          host: https://localhost:9200
          secure: true
          verify_certs: false
          username: admin
        catalog_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/knowledge-flow/catalog.duckdb"
        prompt_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/knowledge-flow/prompt.duckdb"
        resource_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/knowledge-flow/resource.duckdb"
        kpi_store:
          type: "log"
          level: "INFO"
        tag_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/knowledge-flow/tag.duckdb"
        metadata_store:
          type: "duckdb"
          duckdb_path: "/home/fred-user/.fred/knowledge-flow/metadata.duckdb"

        vector_store:
          type: in_memory
        tabular_stores:
          base_database: # Minimal configuration to enable CSV document ingestion
            type: "sql"
            driver: "duckdb"
            database: "base_database"
            path: "/home/fred-user/.fred/knowledge-flow/db.duckdb"
            mode: "read_and_write"
      mcp:
        reports_enabled: true
        kpi_enabled: true
        tabular_enabled: true
        statistic_enabled: true
        text_enabled: true
        templates_enabled: true
        resources_enabled: true
        filesystem_enabled: false
        opensearch_ops_enabled: false
        neo4j_enabled: false

      filesystem:
        type: local
        # type: minio
        # endpoint: http://localhost:9000
        # access_key: admin
        # bucket_name: filesystem
        # secure: false
    dotenv:
      KEYCLOAK_KNOWLEDGE_FLOW_CLIENT_SECRET: ""
      MINIO_SECRET_KEY: ""
      OPENAI_API_KEY: ""
      OPENSEARCH_PASSWORD: ""
      OPENFGA_API_TOKEN: ""
      FRED_POSTGRES_PASSWORD: ""
      TABULAR_POSTGRES_PASSWORD: ""
