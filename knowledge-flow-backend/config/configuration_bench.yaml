# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

app:
  name: "Knowledge Flow Backend"
  base_url: "/knowledge-flow/v1"
  address: "127.0.0.1"
  port: 8111
  log_level: "warning"
  reload: false
  reload_dir: "."
  metrics_enabled: false
  metrics_address: "0.0.0.0"
  metrics_port: 9111
  kpi_process_metrics_interval_sec: 10
  kpi_log_summary_interval_sec: 10.0
  kpi_log_summary_top_n: 20
  max_ingestion_workers: 3

processing:
  generate_summary: false
  use_gpu: true
  process_images: false

security:
  m2m:
    enabled: false
    client_id: "knowledge-flow"
    realm_url: "http://app-keycloak:8080/realms/app"
    secret_env_var: KEYCLOAK_KNOWLEDGE_FLOW_CLIENT_SECRET # pragma: allowlist secret
  user:
    enabled: false
    client_id: "app"
    realm_url: "http://app-keycloak:8080/realms/app"
  authorized_origins:
    - "http://localhost:5173"
  rebac:
    enabled: false
    type: openfga
    api_url: "http://localhost:9080"

scheduler:
  enabled: true
  backend: "temporal"
  temporal:
    host: "localhost:7233"
    namespace: "default"
    task_queue: "ingestion"
    workflow_id_prefix: "pipeline"
    connect_timeout_seconds: 5

input_processors:
  - prefix: ".pdf"
    class_path: knowledge_flow_backend.core.processors.input.pdf_markdown_processor.pdf_markdown_processor.PdfMarkdownProcessor
  - prefix: ".docx"
    class_path: knowledge_flow_backend.core.processors.input.docx_markdown_processor.docx_markdown_processor.DocxMarkdownProcessor
  - prefix: ".pptx"
    class_path: knowledge_flow_backend.core.processors.input.pptx_markdown_processor.pptx_markdown_processor.PptxMarkdownProcessor
  - prefix: ".csv"
    class_path: knowledge_flow_backend.core.processors.input.csv_tabular_processor.csv_tabular_processor.CsvTabularProcessor
  - prefix: ".txt"
    class_path: knowledge_flow_backend.core.processors.input.text_markdown_processor.text_markdown_processor.TextMarkdownProcessor
  - prefix: ".md"
    class_path: knowledge_flow_backend.core.processors.input.markdown_markdown_processor.markdown_markdown_processor.MarkdownMarkdownProcessor
  - prefix: ".xlsm"
    class_path: knowledge_flow_backend.core.processors.input.pps_tabular_processor.pps_tabular_processor.PpsTabularProcessor
  - prefix: ".jsonl"
    class_path: knowledge_flow_backend.core.processors.input.jsonl.jsonl_markdown_processor.JsonlMarkdownProcessor

# Optional: fast processors used for chat attachments (/fast/text, /fast/ingest).
# If omitted, defaults to the Unstructured fast processor.
attachment_processors:
  - prefix: "*"
    class_path: knowledge_flow_backend.core.processors.input.fast_text_processor.fast_unstructured_text_processor.FastUnstructuredTextProcessingProcessor

content_storage:
  type: minio
  endpoint: http://localhost:9000
  access_key: "admin"
  bucket_name: "knowledge-flow-content"
  secure: false

document_sources:
  fred:
    type: push
    description: "Documents manually uploaded by users"

chat_model:
  provider: openai
  name: gpt-4o-mini # any chat-capable model (gpt-4o, gpt-4o-mini, gpt-4.1, etc.)
  settings:
    temperature: 0
    max_retries: 0
    timeout:
      connect: 10.0
      read: 30.0
    http_client_limits:
      max_connections: 500
      max_keepalive_connections: 0
      keepalive_expiry_seconds: 10
embedding_model:
  provider: openai
  name: text-embedding-3-large # or text-embedding-3-small
  settings:
    max_retries: 0
    timeout:
      connect: 10.0
      read: 30.0
    http_client_limits:
      max_connections: 500
      max_keepalive_connections: 0
      keepalive_expiry_seconds: 10

vision_model:
  provider: openai
  name: gpt-4o-mini # or gpt-4o if you want higher fidelity
  settings:
    max_retries: 0
    timeout:
      connect: 10.0
      read: 30.0
    http_client_limits:
      max_connections: 500
      max_keepalive_connections: 0
      keepalive_expiry_seconds: 10
    temperature: 0 # optional
crossencoder_model:
  name: cross-encoder/ms-marco-MiniLM-L-12-v2
  settings:
    online: true
    local_path: ~/.cache/huggingface/hub
    max_retries: 0
    timeout:
      connect: 10.0
      read: 30.0
    http_client_limits:
      max_connections: 500
      max_keepalive_connections: 0
      keepalive_expiry_seconds: 10

# chat_model:
#   provider: ollama
#   name: qwen2.5:3b-instruct         # any chat-capable Ollama model you pulled
#   settings:
#     base_url: http://localhost:11434
#     temperature: 0.7                # optional, forwarded to LangChain Ollama wrapper

# embedding_model:
#   provider: ollama
#   name: nomic-embed-text            # embedding model served by Ollama
#   settings:
#     base_url: http://localhost:11434

# chat_model:
#   provider: azure
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: fred-gpt-4o              # Azure deployment name for your chat model
#   settings:
#     azure_endpoint: https://<your-azure-openai>.openai.azure.com
#     azure_openai_api_version: 2024-05-01-preview
# embedding_model:
#   provider: azure
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: fred-embed-3             # Azure deployment name for your embedding model
#   settings:
#     azure_endpoint: https://<your-azure-openai>.openai.azure.com
#     azure_openai_api_version: 2024-05-01-preview

# embedding_model:
#   provider: azure-apim
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: fred-text-embedding-3-large
#   settings:
#     azure_ad_client_id: "your-client-id"
#     azure_ad_client_scope: "api://your-client-id/.default"
#     azure_apim_base_url: "https://trustnest.azure-api.net"
#     azure_apim_resource_path: "/genai-aoai-inference/v1"
#     azure_openai_api_version: "2024-06-01"
#     azure_tenant_id: "your-tenant-id"

# chat_model:
#   provider: azure-apim        # openai | azure | ollama
#   # azure use deployment name instead of a plain model name as OpenAI does. The principle
#   # is the same.
#   name: gpt-4o
#   settings:
#     azure_ad_client_id: "your-client-id"
#     azure_ad_client_scope: "api://your-client-id/.default"
#     azure_apim_base_url: "https://trustnest.azure-api.net"
#     azure_apim_resource_path: "/genai-aoai-inference/v2"
#     azure_openai_api_version: "2024-06-01"
#     azure_tenant_id: "your-tenant-id"

storage:
  tabular_stores:
    base_database:
      type: "sql"
      mode: "read_and_write"
      driver: "postgresql+psycopg2"
      host: "localhost"
      port: 5432
      database: "data"
      path: null # (important: remove the `path` default value)
      username: "tabular"

  postgres:
    host: localhost
    port: 5432
    database: fred
    username: fred
    pool_size: 2
    max_overflow: 0
    pool_timeout: 30
    pool_recycle: 1800
    pool_pre_ping: true

  opensearch:
    host: https://localhost:9200
    secure: true
    verify_certs: false
    username: admin

  catalog_store:
    type: "postgres"
    table: catalog

  prompt_store:
    type: "postgres"
    table: prompt

  resource_store:
    type: "postgres"
    table: resource

  kpi_store:
    type: "opensearch"
    index: kpi-index

  tag_store:
    type: "postgres"
    table: tag

  metadata_store:
    type: "postgres"
    table: metadata
  task_store:
    type: "postgres"
    table: workflow_tasks
    prefix: "sched_"

  vector_store:
    type: opensearch
    index: vector-index
    # Default bulk size for vector insertions is 500. Increase if you have
    # enough memory and want faster ingestion. Or if run into ingestion errors
    # while ingesting large documents.
    bulk_size: 1000

  log_store:
    type: "opensearch"
    index: log-index

mcp:
  reports_enabled: true
  kpi_enabled: true
  tabular_enabled: true
  statistic_enabled: true
  text_enabled: true
  templates_enabled: true
  resources_enabled: true
  opensearch_ops_enabled: true
  neo4j_enabled: false
  filesystem_enabled: true

filesystem:
  type: minio
  endpoint: http://localhost:9000
  access_key: admin
  bucket_name: filesystem
  secure: false

# Workspace storage layout (paths inside the bucket/filesystem)
workspace_layout:
  user_pattern: "users/{user_id}/{key}"
  agent_config_pattern: "agents/{agent_id}/config/{key}"
  agent_user_pattern: "agents/{agent_id}/users/{user_id}/{key}"
