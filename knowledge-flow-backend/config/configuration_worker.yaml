app:
  name: "Knowledge Flow Backend"
  base_url: "/knowledge-flow/v1"
  address: "127.0.0.1"
  port: 8112
  log_level: "info"
  reload: false
  reload_dir: "."

security:
  m2m:
    enabled: false
    client_id: "knowledge-flow"
    realm_url: "http://app-keycloak:8080/realms/app"
  user:
    enabled: false
    client_id: "app"
    realm_url: "http://app-keycloak:8080/realms/app"
  authorized_origins:
    - "http://localhost:5173"
  rebac:
    enabled: false
    type: openfga
    api_url: "http://localhost:9080"

scheduler:
  enabled: true
  backend: "temporal"
  temporal:
    host: "localhost:7233"
    namespace: "default"
    task_queue: "ingestion"
    workflow_id_prefix: "pipeline"
    connect_timeout_seconds: 5

# -----------------------------------------------------------------------------
# INPUT PROCESSORS
# -----------------------------------------------------------------------------
# Mandatory: Input processors MUST be explicitly defined.
# These classes parse incoming files (e.g., PDFs, DOCXs, CSVs) into structured documents.

input_processors:
  - prefix: ".pdf"
    class_path: knowledge_flow_backend.core.processors.input.pdf_markdown_processor.pdf_markdown_processor.PdfMarkdownProcessor
  - prefix: ".docx"
    class_path: knowledge_flow_backend.core.processors.input.docx_markdown_processor.docx_markdown_processor.DocxMarkdownProcessor
  - prefix: ".pptx"
    class_path: knowledge_flow_backend.core.processors.input.pptx_markdown_processor.pptx_markdown_processor.PptxMarkdownProcessor
  - prefix: ".csv"
    class_path: knowledge_flow_backend.core.processors.input.csv_tabular_processor.csv_tabular_processor.CsvTabularProcessor
  - prefix: ".txt"
    class_path: knowledge_flow_backend.core.processors.input.text_markdown_processor.text_markdown_processor.TextMarkdownProcessor
  - prefix: ".md"
    class_path: knowledge_flow_backend.core.processors.input.markdown_markdown_processor.markdown_markdown_processor.MarkdownMarkdownProcessor
  - prefix: ".xlsm"
    class_path: knowledge_flow_backend.core.processors.input.pps_tabular_processor.pps_tabular_processor.PpsTabularProcessor

# Optional: fast processors used for chat attachments (/fast/text, /fast/ingest).
# If omitted, defaults to the Unstructured fast processor.
attachment_processors:
  - prefix: "*"
    class_path: knowledge_flow_backend.core.processors.input.fast_text_processor.fast_unstructured_text_processor.FastUnstructuredTextProcessingProcessor

content_storage:
  type: minio
  endpoint: http://localhost:9000
  access_key: "admin"
  bucket_name: "knowledge-flow-content"
  secure: false

document_sources:
  fred:
    type: push
    description: "Documents manually uploaded by users"

chat_model:
  provider: openai
  name: gpt-4o-mini # any chat-capable model (gpt-4o, gpt-4o-mini, gpt-4.1, etc.)
  settings:
    temperature: 0
    max_retries: 0
    timeout:
      connect: 10.0
      read: 30.0
    http_client_limits:
      max_connections: 500
      max_keepalive_connections: 0
      keepalive_expiry_seconds: 10
embedding_model:
  provider: openai
  name: text-embedding-3-large # or text-embedding-3-small
  settings:
    max_retries: 0
    timeout:
      connect: 10.0
      read: 30.0
    http_client_limits:
      max_connections: 500
      max_keepalive_connections: 0
      keepalive_expiry_seconds: 10

vision_model:
  provider: openai
  name: gpt-4o-mini # or gpt-4o if you want higher fidelity
  settings:
    max_retries: 0
    timeout:
      connect: 10.0
      read: 30.0
    http_client_limits:
      max_connections: 500
      max_keepalive_connections: 0
      keepalive_expiry_seconds: 10
    temperature: 0 # optional
crossencoder_model:
  name: cross-encoder/ms-marco-MiniLM-L-12-v2
  settings:
    online: true
    local_path: ~/.cache/huggingface/hub
    max_retries: 0
    timeout:
      connect: 10.0
      read: 30.0
    http_client_limits:
      max_connections: 500
      max_keepalive_connections: 0
      keepalive_expiry_seconds: 10

storage:
  postgres:
    host: localhost
    port: 5432
    database: fred
    username: fred

  opensearch:
    host: https://localhost:9200
    secure: true
    verify_certs: false
    username: admin

  catalog_store:
    type: "postgres"
    table: catalog

  prompt_store:
    type: "postgres"
    table: prompt

  resource_store:
    type: "postgres"
    table: resource

  kpi_store:
    type: "opensearch"
    index: kpi-index

  tag_store:
    type: "postgres"
    table: tag

  metadata_store:
    type: "postgres"
    table: metadata

  vector_store:
    type: opensearch
    index: vector-index
    # Default bulk size for vector insertions is 500. Increase if you have
    # enough memory and want faster ingestion. Or if run into ingestion errors
    # while ingesting large documents.
    bulk_size: 1000

  log_store:
    type: "opensearch"
    index: log-index

  tabular_stores:
    base_database:
      type: "sql"
      mode: "read_and_write"
      driver: "postgresql+psycopg2"
      host: "localhost"
      port: 5432
      database: "data"
      path: null # (important: remove the `path` default value)
      username: "tabular"

mcp:
  reports_enabled: true
  kpi_enabled: true
  tabular_enabled: true
  statistic_enabled: true
  text_enabled: true
  templates_enabled: true
  resources_enabled: true
  opensearch_ops_enabled: true
  neo4j_enabled: false
  filesystem_enabled: true

filesystem:
  type: minio
  endpoint: http://localhost:9000
  access_key: admin
  bucket_name: filesystem
  secure: false

# Workspace storage layout (paths inside the bucket/filesystem)
workspace_layout:
  user_pattern: "users/{user_id}/{key}"
  agent_config_pattern: "agents/{agent_id}/config/{key}"
  agent_user_pattern: "agents/{agent_id}/users/{user_id}/{key}"
