# -----------------------------------------------------------------------------
# BUILD STAGE
# -----------------------------------------------------------------------------
FROM mirror.gcr.io/python:3.12.8-slim AS builder
ARG USER_NAME=fred-user
ARG USER_ID=1000
ARG GROUP_ID=1000
ARG UV_VERSION=0.7.21

# Install system deps required for build
RUN apt-get update && \
    apt-get install -y git curl make && \
    rm -rf /var/lib/apt/lists/*

# Install uv globally
RUN curl -LsSf https://astral.sh/uv/${UV_VERSION}/install.sh | sh && \
    mv /root/.local/bin/uv /usr/local/bin/uv && \
    chmod +x /usr/local/bin/uv

# Create virtualenv manually (outside project for portability)
ENV UV_PROJECT_ENVIRONMENT=/app/venv
ENV PATH="/app/venv/bin:$PATH"
ENV TIKTOKEN_CACHE_DIR=/app/encodings

# Set working directory
WORKDIR /app

# Copy python dependencies files
COPY knowledge-flow-backend/pyproject.toml knowledge-flow-backend/uv.lock* /app/
# Copy Makefile-related files
COPY knowledge-flow-backend/Makefile /app/
COPY scripts /scripts

# Copy fred-core folder as this is a Python-dependency for the backend
COPY fred-core /fred-core

# Install dependencies using make with both VENV and TARGET properly set
RUN make dev

# Prepare local encodings directory
RUN mkdir -p $TIKTOKEN_CACHE_DIR

# Download encodings for offline usage
RUN python /scripts/download_encodings.py https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

# Copy projets
COPY knowledge-flow-backend/. /app/

# -----------------------------------------------------------------------------
# FINAL STAGE: PROD IMAGE
# -----------------------------------------------------------------------------
FROM mirror.gcr.io/python:3.12.8-slim
ARG USER_NAME=fred-user
ARG USER_ID=1000
ARG GROUP_ID=1000
ARG HF_OFFLINE_STRICT=1

# System deps (minimal)
RUN apt-get update && \
    apt-get install -y \
    curl \
    pandoc \
    inkscape \
    libgl1 \
    libmagic1 \
    poppler-utils \
    ffmpeg \
    tesseract-ocr \
    tesseract-ocr-eng \
    tesseract-ocr-fra \
    tesseract-ocr-osd \
    --no-install-recommends && \
    rm -rf /var/lib/apt/lists/*

# Create user
RUN groupadd -g ${GROUP_ID} ${USER_NAME} && \
    useradd -u ${USER_ID} -g ${GROUP_ID} -m ${USER_NAME} && \
    mkdir -p /home/${USER_NAME}/.fred/knowledge-flow && \
    chown -R ${USER_NAME}: /home/${USER_NAME}

# Copy virtualenv from build stage
COPY --from=builder --chown=${USER_ID}:${GROUP_ID} /app/venv /app/venv
COPY --from=builder --chown=${USER_ID}:${GROUP_ID} /app/encodings /app/encodings

ENV PATH="/app/venv/bin:$PATH"
ENV TIKTOKEN_CACHE_DIR=/app/encodings
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# Cache policy:
# - Keep all mutable ML caches under /app/.cache (explicit, easy to inspect).
# - Make cache tree writable for arbitrary runtime UID/GID.
ENV APP_CACHE_DIR=/app/.cache
ENV XDG_CACHE_HOME=${APP_CACHE_DIR}
ENV HF_HOME=${APP_CACHE_DIR}/huggingface
ENV HUGGINGFACE_HUB_CACHE=${APP_CACHE_DIR}/huggingface/hub
ENV HF_HUB_CACHE=${APP_CACHE_DIR}/huggingface/hub
ENV TRANSFORMERS_CACHE=${APP_CACHE_DIR}/huggingface/transformers
ENV SENTENCE_TRANSFORMERS_HOME=${APP_CACHE_DIR}/huggingface
ENV TORCH_HOME=${APP_CACHE_DIR}/torch
ENV EASYOCR_MODULE_PATH=${APP_CACHE_DIR}/easyocr
ENV DOCLING_ARTIFACTS_PATH=${APP_CACHE_DIR}/docling/models
RUN mkdir -p ${APP_CACHE_DIR}/huggingface/hub ${APP_CACHE_DIR}/huggingface/transformers ${APP_CACHE_DIR}/torch ${APP_CACHE_DIR}/easyocr ${DOCLING_ARTIFACTS_PATH} && \
    chmod -R 0777 ${APP_CACHE_DIR}

# Hugging Face / reranker cache layout
ENV RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-12-v2
USER ${USER_NAME}
RUN python - <<'PY'
import os
from sentence_transformers import CrossEncoder

CrossEncoder(
    model_name_or_path=os.environ["RERANKER_MODEL"],
    cache_folder=os.environ["HF_HOME"],
)
PY

# Preload Docling artifacts needed by PDF medium/rich profiles:
# - layout (dlparse_v4)
# - tableformer (rich with do_table_structure=true)
# - easyocr (rich with do_ocr=true)
RUN python - <<'PY'
import os
from pathlib import Path
from docling.utils.model_downloader import download_models

output_dir = Path(os.environ["DOCLING_ARTIFACTS_PATH"]).expanduser()
output_dir.mkdir(parents=True, exist_ok=True)
download_models(
    output_dir=output_dir,
    force=False,
    progress=False,
    with_layout=True,
    with_tableformer=True,
    with_code_formula=False,
    with_picture_classifier=False,
    with_smolvlm=False,
    with_smoldocling=False,
    with_smoldocling_mlx=False,
    with_granite_vision=False,
    with_easyocr=True,
)
print(f"Docling artifacts ready in: {output_dir}")
PY
USER root
RUN chmod -R 0777 ${APP_CACHE_DIR}

# Runtime offline policy for Hugging Face based dependencies.
ENV HF_HUB_OFFLINE=${HF_OFFLINE_STRICT}
ENV TRANSFORMERS_OFFLINE=${HF_OFFLINE_STRICT}
ENV HF_HUB_DISABLE_TELEMETRY=1

# Set working directory
WORKDIR /app

# Copy app source
COPY --chown=${USER_ID}:${GROUP_ID} knowledge-flow-backend/. /app
COPY --chown=${USER_ID}:${GROUP_ID} fred-core /fred-core
COPY --chown=${USER_ID}:${GROUP_ID} scripts /scripts

# Set environment
ENV PYTHONPATH=/app

# Switch to non-root user
USER ${USER_NAME}

# Expose Fast API port
EXPOSE 8111

# Entrypoint without make
ENTRYPOINT ["uvicorn", "knowledge_flow_backend.main:create_app", "--factory"]
CMD ["--port", "8111", "--host", "0.0.0.0", "--log-level", "info", "--loop", "asyncio"]
