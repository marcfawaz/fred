# -----------------------------------------------------------------------------
# BUILD STAGE
# -----------------------------------------------------------------------------
FROM mirror.gcr.io/python:3.12.8-slim AS builder
ARG USER_NAME=fred-user
ARG USER_ID=1000
ARG GROUP_ID=1000
ARG UV_VERSION=0.7.21

# Install system deps required for build
RUN apt-get update && \
    apt-get install -y git curl make && \
    rm -rf /var/lib/apt/lists/*

# Install uv globally
RUN curl -LsSf https://astral.sh/uv/${UV_VERSION}/install.sh | sh && \
    mv /root/.local/bin/uv /usr/local/bin/uv && \
    chmod +x /usr/local/bin/uv

# Create virtualenv manually (outside project for portability)
ENV UV_PROJECT_ENVIRONMENT=/app/venv
ENV PATH="/app/venv/bin:$PATH"
ENV TIKTOKEN_CACHE_DIR=/app/encodings

# Set working directory
WORKDIR /app

# Copy python dependencies files
COPY knowledge-flow-backend/pyproject.toml knowledge-flow-backend/uv.lock* /app/
# Copy Makefile-related files
COPY knowledge-flow-backend/Makefile /app/
COPY scripts /scripts

# Copy fred-core folder as this is a Python-dependency for the backend
COPY fred-core /fred-core

# Install dependencies using make with both VENV and TARGET properly set
RUN make dev

# Prepare local encodings directory
RUN mkdir -p $TIKTOKEN_CACHE_DIR

# Download encodings for offline usage
RUN python /scripts/download_encodings.py https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

# Copy projets
COPY knowledge-flow-backend/. /app/

# -----------------------------------------------------------------------------
# FINAL STAGE: PROD IMAGE
# -----------------------------------------------------------------------------
FROM mirror.gcr.io/python:3.12.8-slim
ARG USER_NAME=fred-user
ARG USER_ID=1000
ARG GROUP_ID=1000

# System deps (minimal)
RUN apt-get update && \
    apt-get install -y \
    curl \
    pandoc \
    inkscape \
    libgl1 \
    libmagic1 \
    poppler-utils \
    ffmpeg \
    tesseract-ocr \
    tesseract-ocr-eng \
    tesseract-ocr-fra \
    tesseract-ocr-osd \
    --no-install-recommends && \
    rm -rf /var/lib/apt/lists/*

# Create user
RUN groupadd -g ${GROUP_ID} ${USER_NAME} && \
    useradd -u ${USER_ID} -g ${GROUP_ID} -m ${USER_NAME} && \
    mkdir -p /home/${USER_NAME}/.fred/knowledge-flow && \
    chown -R ${USER_NAME}: /home/${USER_NAME}

# Copy virtualenv from build stage
COPY --from=builder --chown=${USER_ID}:${GROUP_ID} /app/venv /app/venv
COPY --from=builder --chown=${USER_ID}:${GROUP_ID} /app/encodings /app/encodings

ENV PATH="/app/venv/bin:$PATH"
ENV TIKTOKEN_CACHE_DIR=/app/encodings
ENV DOCLING_ARTIFACTS_PATH=/app/docling-artifacts

# Hugging Face / reranker cache layout
ENV CACHE_HOME=/home/${USER_NAME}/.cache
ENV RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-12-v2
ENV RERANKER_MODEL_CACHE=${CACHE_HOME}/huggingface
ENV HF_HOME=${CACHE_HOME}
ENV HF_HUB_CACHE=${RERANKER_MODEL_CACHE}
ENV HUGGINGFACE_HUB_CACHE=${RERANKER_MODEL_CACHE}
ENV SENTENCE_TRANSFORMERS_HOME=${RERANKER_MODEL_CACHE}
ENV TRANSFORMERS_CACHE=${RERANKER_MODEL_CACHE}
RUN mkdir -p ${CACHE_HOME} ${RERANKER_MODEL_CACHE} ${DOCLING_ARTIFACTS_PATH} && \
    chown -R ${USER_ID}:${GROUP_ID} ${CACHE_HOME} ${DOCLING_ARTIFACTS_PATH}

# Warm the cross-encoder cache during build (faster startup, offline-friendly runtime)
USER ${USER_NAME}
RUN python -c "\
from sentence_transformers import CrossEncoder; \
import os; \
CrossEncoder(model_name_or_path=os.environ['RERANKER_MODEL'], cache_folder=os.environ['RERANKER_MODEL_CACHE'])"

# Preload Docling artifacts (layout/table + EasyOCR) for offline-friendly runtime
RUN python -c "\
from pathlib import Path; \
import os; \
from docling.utils.model_downloader import download_models; \
download_models( \
    output_dir=Path(os.environ['DOCLING_ARTIFACTS_PATH']), \
    progress=False, \
    with_layout=True, \
    with_tableformer=True, \
    with_easyocr=True, \
    with_code_formula=False, \
    with_picture_classifier=False, \
    with_smolvlm=False, \
    with_smoldocling=False, \
    with_smoldocling_mlx=False, \
    with_granite_vision=False \
)"

# Set working directory
WORKDIR /app

# Copy app source
COPY --chown=${USER_ID}:${GROUP_ID} knowledge-flow-backend/. /app
COPY --chown=${USER_ID}:${GROUP_ID} fred-core /fred-core
COPY --chown=${USER_ID}:${GROUP_ID} scripts /scripts

# Set environment
ENV PYTHONPATH=/app

# Expose Fast API port
EXPOSE 8111

# Entrypoint without make
ENTRYPOINT ["uvicorn", "knowledge_flow_backend.main:create_app", "--factory"]
CMD ["--port", "8111", "--host", "0.0.0.0", "--log-level", "info", "--loop", "asyncio"]
