# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

app:
  name: "Agentic Backend"
  base_url: "/agentic/v1"
  address: "127.0.0.1"
  port: 8000
  log_level: "debug"
  reload: false
  reload_dir: "."
  metrics_enabled: true
  metrics_address: "0.0.0.0"
  metrics_port: 9005 # (not 9000 to avoid conflict with Minio)
  # KPI settings for local benchmarks and debugging.
  kpi_process_metrics_interval_sec: 10
  kpi_log_summary_interval_sec: 10.0
  kpi_log_summary_top_n: 20

scheduler:
  enabled: true
  backend: temporal
  temporal:
    host: "localhost:7233"
    namespace: "default"
    task_queue: "agents"

security:
  m2m:
    enabled: false
    client_id: "agentic"
    realm_url: "http://app-keycloak:8080/realms/app"
    secret_env_var: KEYCLOAK_AGENTIC_CLIENT_SECRET # pragma: allowlist secret (env var name only)
  user:
    enabled: false
    client_id: "app"
    realm_url: "http://app-keycloak:8080/realms/app"
  authorized_origins:
    - "http://localhost:5173"
  rebac:
    enabled: false
    type: openfga
    api_url: "http://localhost:9080"

frontend_settings:
  feature_flags:
    # If true activate the backend and frontend modules in charge of K8
    # and frugality monitoring
    enableK8Features: false
    # If true activate support for an electronic warfare demonstration
    enableElecWarfare: false
  properties:
    logoName: "fred"
    siteDisplayName: "Fred"

ai:
  # In production  activate the configuration persistence mechanism
  # that allow users to create/update agents at runtime and change
  # their configuration without redeploying the application.
  # Developers: you may prefer to set this to true to trust only your static configuration
  use_static_config_only: false

  # Number of past exchanges to restore when initializing an agent session
  # This setting is used to limit the amount of historical context
  # that is loaded into the agent at the beginning of a session. It typically occur after a restart of the application
  # or when an agent session is re-initialized for any reason.
  agentic_restore_max_exchanges: 20

  # Maximum number of agents that cached in memory for faster access (uses LRU eviction policy)
  max_concurrent_agents: 1024
  # Maximum number of concurrent sessions per user. This is used to prevent abuse and manage resource usage.
  max_concurrent_sessions_per_user: 10

  # Maximum number of files a user can attach across all sessions
  max_attached_files_per_user: 20

  # Maximum size (in MB) for each attached file
  max_attached_file_size_mb: 50

  # Base URL for the Knowledge Flow service
  knowledge_flow_url: "http://localhost:8111/knowledge-flow/v1"

  # Timeout settings for the client (used for Knowledge Flow calls, including PDF/text extraction).
  timeout:
    connect: 20 # Time to wait for a connection in seconds
    read: 60 # Time to wait for a response in seconds (longer for large document processing)

  default_chat_model:
    provider: "openai"
    name: "model-2"
    settings:
      max_retries: 0
      # Benchmark (mock OpenAI): uncomment the base_url below to target the mock server.
      base_url: "http://localhost:8383/v1"
      timeout:
        connect: 20.0
        read: 60.0
        write: 60.0
        pool: 20.0
      http_client_limits:
        max_connections: 500
        max_keepalive_connections: 200
        keepalive_expiry_seconds: 10
      temperature: 0.0

  # Default language model used for non-chat interactions. This one is optional
  # and if not set, the default_chat_model will be used as fallback.
  default_language_model:
    # Required in .env:
    # - OPENAI_API_KEY
    provider: "openai"
    name: "model-2"
    settings:
      # Benchmark (mock OpenAI): uncomment the base_url below to target the mock server.
      base_url: "http://localhost:8383/v1"
      temperature: 0.0
      max_retries: 0
      timeout:
        connect: 10.0
        read: 30.0
        write: 30.0
        pool: 5.0
      http_client_limits:
        max_connections: 500
        max_keepalive_connections: 200
        keepalive_expiry_seconds: 10

  recursion:
    recursion_limit: 40 # Number or max recursion use by the agents while using the chat_model
  agents:
    - name: "Georges"
      type: "agent"
      class_path: "agentic_backend.agents.generalist.generalist_expert.Georges"
      enabled: true
      chat_options:
        attach_files: false

mcp:
  servers:
    - id: "mcp-knowledge-flow-mcp-text"
      name: "mcp.servers.search_documents.name"
      description: "mcp.servers.search_documents.description"
      transport: "streamable_http"
      url: "http://localhost:8111/knowledge-flow/v1/mcp-text"
      sse_read_timeout: 2000
      auth_mode: "user_token"
    - id: "mcp-knowledge-flow-mcp-tabular"
      name: "mcp.servers.tabular.name"
      description: "mcp.servers.tabular.description"
      transport: "streamable_http"
      url: "http://localhost:8111/knowledge-flow/v1/mcp-tabular"
      sse_read_timeout: 2000
      auth_mode: "user_token"
    - id: "mcp-knowledge-flow-opensearch-ops"
      name: "mcp.servers.search_opensearch.name"
      description: "mcp.servers.search_opensearch.description"
      transport: "streamable_http"
      url: "http://localhost:8111/knowledge-flow/v1/mcp-opensearch-ops"
      sse_read_timeout: 2000
      auth_mode: "user_token"
    - id: "mcp-kubernetes-server"
      name: "mcp.servers.kubernetes.name"
      description: "mcp.servers.kubernetes.description"
      enabled: false
      transport: "streamable_http"
      url: "http://localhost:8081/mcp"
      sse_read_timeout: 2000
      auth_mode: "user_token"
    - id: "mcp-knowledge-flow-fs"
      name: "mcp.servers.filesystem.name"
      description: "mcp.servers.filesystem.description"
      enabled: true
      transport: "streamable_http"
      url: "http://localhost:8111/knowledge-flow/v1/mcp-fs"
      sse_read_timeout: 2000
      auth_mode: "user_token"
    - id: "mcp-neo4j-graph-db"
      name: "mcp.servers.neo4j.name"
      description: "mcp.servers.neo4j.description"
      transport: "streamable_http"
      url: "http://localhost:8111/knowledge-flow/v1/mcp-neo4j"
      sse_read_timeout: 2000
      auth_mode: "user_token"

storage:
  postgres:
    host: localhost
    port: 5432
    database: fred
    username: fred
    pool_size: 20
    max_overflow: 20
    pool_timeout: 30
    pool_recycle: 1800

  opensearch:
    host: https://localhost:9200
    secure: true
    verify_certs: false
    username: admin

  feedback_store:
    type: "postgres"
    table: feedbacks

  agent_store:
    type: "postgres"
    table: agent

  mcp_servers_store:
    type: "postgres"
    table: mcp-server

  session_store:
    type: "postgres"
    table: session

  attachments_store:
    type: "postgres"
    table: session_attachments

  history_store:
    type: "postgres"
    table: session_history

  task_store:
    type: "postgres"
    table: tasks

  kpi_store:
    type: "opensearch"
    index: kpi-index

  log_store:
    type: "opensearch"
    index: log-index
